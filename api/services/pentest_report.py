"""
Pentest Report Generator â€” Genera reporte markdown y JSON
"""

import logging
from pathlib import Path
from typing import List
from api.models.pentest import PentestTaskResult, PentestExecutionResults

logger = logging.getLogger(__name__)


def generate_pentest_report(
    pentest_id: str,
    case_id: str,
    completed_tasks: List[PentestTaskResult],
    failed_tasks: List[PentestTaskResult],
    evidence_base_path: Path,
) -> PentestExecutionResults:
    """
    Genera reporte markdown y JSON de resultados.
    Persiste en directorio de evidencia.
    """
    
    report_dir = evidence_base_path.parent / "report"
    report_dir.mkdir(parents=True, exist_ok=True)
    
    total = len(completed_tasks) + len(failed_tasks)
    success_rate = (len(completed_tasks) / total * 100) if total > 0 else 0
    
    # Crear objeto de resultados
    results = PentestExecutionResults(
        pentest_id=pentest_id,
        case_id=case_id,
        total_tasks=total,
        completed_tasks=completed_tasks,
        failed_tasks=failed_tasks,
        success_rate=success_rate,
        evidence_base_path=str(evidence_base_path),
    )
    
    # Guardar JSON
    json_path = report_dir / "results.json"
    json_path.write_text(results.model_dump_json(indent=2))
    logger.info(f"ğŸ“„ Resultados guardados en {json_path}")
    
    # Generar markdown
    md_path = report_dir / "pentest_report.md"
    md_content = _generate_markdown_report(results)
    md_path.write_text(md_content)
    logger.info(f"ğŸ“„ Reporte guardado en {md_path}")
    
    return results


def _generate_markdown_report(results: PentestExecutionResults) -> str:
    """Genera contenido markdown del reporte."""
    
    lines = [
        f"# Pentest Report {results.pentest_id}",
        f"**Case:** {results.case_id}",
        f"**Generated:** {results.executed_at.isoformat()}Z",
        "",
        "## Summary",
        f"- **Total Tasks:** {results.total_tasks}",
        f"- **Completed:** {len(results.completed_tasks)}",
        f"- **Failed:** {len(results.failed_tasks)}",
        f"- **Success Rate:** {results.success_rate:.1f}%",
        "",
        "## Completed Tasks",
    ]
    
    if results.completed_tasks:
        for task in results.completed_tasks:
            lines.append(f"- âœ… {task.task_id} ({task.tool}) - {task.duration_seconds:.1f}s")
            if task.output:
                lines.append(f"  - Output: {task.output[:100]}...")
    else:
        lines.append("*No completed tasks*")
    
    lines.extend([
        "",
        "## Failed Tasks",
    ])
    
    if results.failed_tasks:
        for task in results.failed_tasks:
            lines.append(f"- âŒ {task.task_id} ({task.tool})")
            if task.error:
                lines.append(f"  - Error: {task.error}")
    else:
        lines.append("*No failed tasks*")
    
    lines.extend([
        "",
        "## Evidence Path",
        f"```\n{results.evidence_base_path}\n```",
    ])
    
    return "\n".join(lines)

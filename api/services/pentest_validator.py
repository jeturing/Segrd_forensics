"""
Pentest Validator — Valida plan contra policy
Filtra acciones prohibidas, detecta riesgo medio/alto
"""

import logging
from api.models.pentest import PentestPlan, PentestRisk, PentestPolicy

logger = logging.getLogger(__name__)


def validate_plan(plan: PentestPlan, policy: PentestPolicy) -> PentestPlan:
    """
    Valida y filtra el plan según policy.
    - Rechaza acciones en forbidden_actions
    - Rechaza OFFENSIVE si no está autorizado
    - Marca tareas MEDIUM para aprobación
    """
    
    # Convertir policy a dict para facilitar acceso
    policy_dict = policy.model_dump()
    
    allowed_actions = set(a.lower() for a in policy_dict.get("allowed_actions", []))
    forbidden_actions = set(a.lower() for a in policy_dict.get("forbidden_actions", []))
    approval_matrix = policy_dict.get("approval_matrix", {})
    
    valid_tasks = []
    escalation = False
    
    for task in plan.tasks:
        # Usamos 'tool' o 'agent' para validar contra allowed_actions?
        # El prompt del planner usa 'agent' como 'red.recon' y 'tool' como 'nmap'.
        # La policy define allowed_actions como 'recon', 'vuln_scan'.
        # Asumiremos que 'agent' contiene la categoría o que 'tool' es lo que validamos.
        # Por ahora validamos 'tool' contra forbidden_actions si es específico,
        # y 'phase' o 'agent' contra allowed_actions.
        
        # Simplificación: validamos si la herramienta está prohibida explícitamente
        tool = task.tool.lower()
        if tool in forbidden_actions:
            if policy.break_glass_mode:
                logger.warning(f"⚠️ BREAK GLASS: Permitiendo herramienta prohibida {tool} en tarea {task.task_id}")
                # Forzamos riesgo HIGH o OFFENSIVE para asegurar visibilidad/aprobación si aplica
                if task.risk_level not in [PentestRisk.HIGH, PentestRisk.OFFENSIVE]:
                    task.risk_level = PentestRisk.HIGH
            else:
                logger.warning(f"❌ Tarea {task.task_id} usa herramienta prohibida: {tool}")
                continue
            
        # Validar riesgo
        risk = task.risk_level.value if hasattr(task.risk_level, 'value') else str(task.risk_level)
        risk = risk.upper()
        
        # ❌ Rechaza OFFENSIVE si no está autorizado
        if risk == "OFFENSIVE" and not approval_matrix.get("OFFENSIVE", False):
            logger.warning(f"❌ Tarea {task.task_id} es OFFENSIVE pero no autorizado")
            continue
        
        # ⏳ Marca para aprobación si es MEDIUM/HIGH
        if approval_matrix.get(risk, False) or risk in ["MEDIUM", "HIGH"]:
            task.requires_approval = True
            escalation = True
            logger.info(f"⏳ Tarea {task.task_id} requerirá aprobación (risk={risk})")
        
        valid_tasks.append(task)
    
    logger.info(f"✅ Validación completada: {len(valid_tasks)}/{len(plan.tasks)} tareas válidas")
    
    return PentestPlan(
        pentest_id=plan.pentest_id,
        tasks=valid_tasks,
        escalation_required=escalation,
        escalation_reason="Algunas tareas requieren aprobación humana" if escalation else None,
        generated_at=plan.generated_at,
        expires_at=plan.expires_at,
    )

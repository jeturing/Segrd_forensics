"""
Pentest Planner — LLM-based action planning
Respeta policy, nunca ejecuta, genera JSON válido solamente
"""

import json
import logging
from typing import Dict, Any, Optional
from api.config import settings
from api.services.llm_integration import safe_llm_call
from api.models.pentest import PentestPlan, PentestTask, PentestRequest, PentestPolicy

logger = logging.getLogger(__name__)


async def generate_pentest_plan(
    request: PentestRequest,
    policy: PentestPolicy
) -> PentestPlan:
    """
    Wrapper para generar plan desde request y policy.
    """
    # Convertir policy a dict para el prompt
    policy_dict = policy.model_dump()
    
    # Usar un ID temporal para el prompt, el real se asigna fuera
    temp_id = "PLANNING"
    
    return await plan_pentest_with_llm(
        pentest_id=temp_id,
        case_id=request.case_id,
        policy_id=policy.policy_id,
        policy=policy_dict,
        targets=request.targets,
        objectives=request.objectives
    )


async def plan_pentest_with_llm(
    pentest_id: str,
    case_id: str,
    policy_id: str,
    policy: Dict[str, Any],
    targets: Optional[Dict[str, list]] = None,
    objectives: Optional[str] = None,
) -> PentestPlan:
    """
    LLM Planner genera plan de acciones respetando policy.
    Nunca genera comandos de exploit, nunca ejecuta, solo planifica.
    """
    
    prompt = _build_planner_prompt(
        pentest_id=pentest_id,
        case_id=case_id,
        policy=policy,
        targets=targets,
        objectives=objectives,
    )
    
    llm_response = await safe_llm_call(
        prompt=prompt,
        timeout=settings.LLM_STUDIO_TIMEOUT,
        provider_hint="local"
    )
    
    try:
        plan_dict = json.loads(llm_response)
        plan = PentestPlan(
            pentest_id=pentest_id,
            tasks=[PentestTask(**t) for t in plan_dict.get("tasks", [])],
            escalation_required=plan_dict.get("escalation_required", False),
            escalation_reason=plan_dict.get("escalation_reason"),
        )
        logger.info(f"✅ Planner generó plan con {len(plan.tasks)} tareas")
        return plan
    except Exception as e:
        logger.error(f"❌ Planner LLM devolvió JSON inválido: {e}")
        # Fallback: devolver plan vacío en lugar de crash
        return PentestPlan(
            pentest_id=pentest_id,
            tasks=[],
            escalation_required=True,
            escalation_reason=f"Planner error: {str(e)}"
        )


def _build_planner_prompt(
    pentest_id: str,
    case_id: str,
    policy: Dict[str, Any],
    targets: Optional[Dict[str, list]] = None,
    objectives: Optional[str] = None,
) -> str:
    """Construye prompt seguro para el LLM Planner."""
    
    return f"""You are MCP Autonomous Pentest Planner Agent.

STRICT CONSTRAINTS:
- You ONLY plan, you NEVER execute
- You NEVER generate exploit commands
- You NEVER suggest password bruteforce or DoS
- You NEVER exceed policy scope
- You return JSON only

PENTEST_ID: {pentest_id}
CASE_ID: {case_id}

POLICY:
- allowed_actions: {policy.get('allowed_actions', [])}
- forbidden_actions: {policy.get('forbidden_actions', [])}
- intensity: {policy.get('intensity', 'SAFE')}
- scope: {policy.get('scope', {})}

TARGETS: {targets or 'not specified'}
OBJECTIVES: {objectives or 'baseline'}

OUTPUT FORMAT (JSON):
{{
  "tasks": [
    {{
      "task_id": "PT-TASK-001",
      "agent": "red.recon",
      "tool": "nmap",
      "command": ["nmap", "-sV", "target.com"],
      "reason": "Service enumeration within scope",
      "phase": "recon",
      "risk_level": "LOW",
      "requires_approval": false,
      "sandbox": true,
      "timeout": 300
    }}
  ],
  "escalation_required": false,
  "escalation_reason": null
}}
"""

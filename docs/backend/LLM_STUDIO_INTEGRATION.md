# Integraci√≥n LLM Studio + Dynamic Model Manager v4.3

**Estado**: ‚úÖ Completa  
**Fecha**: 7 de diciembre de 2024  
**Versi√≥n**: v4.3  
**Tipo**: Backend + Frontend Integration

---

## üìö Resumen Ejecutivo

Esta implementaci√≥n integra **LLM Studio (Jeturing AI Platform)** con el sistema MCP Kali Forensics, proporcionando capacidades de inteligencia artificial para an√°lisis forense automatizado mediante un sistema de **proveedores m√∫ltiples con fallback autom√°tico**.

### Arquitectura de 3 Niveles

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                   LLM Provider Manager                      ‚îÇ
‚îÇ                      (Singleton)                            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                            ‚îÇ
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ                   ‚îÇ                   ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  LLM Studio    ‚îÇ  ‚îÇ  Phi-4 Local   ‚îÇ  ‚îÇ   Offline   ‚îÇ
‚îÇ   (Primary)    ‚îÇ  ‚îÇ   (Fallback)   ‚îÇ  ‚îÇ (Emergency) ‚îÇ
‚îÇ                ‚îÇ  ‚îÇ                ‚îÇ  ‚îÇ             ‚îÇ
‚îÇ OpenAI-compat  ‚îÇ  ‚îÇ Pattern-based  ‚îÇ  ‚îÇ Rules-based ‚îÇ
‚îÇ Phi-4 Model    ‚îÇ  ‚îÇ Local compute  ‚îÇ  ‚îÇ No AI req.  ‚îÇ
‚îÇ 100.101.115.5  ‚îÇ  ‚îÇ CPU/RAM only   ‚îÇ  ‚îÇ Always avail‚îÇ
‚îÇ                ‚îÇ  ‚îÇ                ‚îÇ  ‚îÇ             ‚îÇ
‚îÇ Timeout: 40s   ‚îÇ  ‚îÇ Fast response  ‚îÇ  ‚îÇ Instant     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Capacidades Implementadas

- ‚úÖ **Multi-Provider Manager**: Gesti√≥n de 3 proveedores LLM con cambio din√°mico
- ‚úÖ **Automatic Fallback**: Si LLM Studio no responde ‚Üí Phi-4 Local ‚Üí Offline
- ‚úÖ **Health Monitoring**: Verificaci√≥n continua de salud de proveedores
- ‚úÖ **Statistics Tracking**: M√©tricas de uso, errores y latencia por proveedor
- ‚úÖ **API REST Completa**: 8 endpoints para configuraci√≥n y gesti√≥n
- ‚úÖ **SOAR Intelligence Integration**: Integrado con motor de an√°lisis forense
- ‚úÖ **React Management Panel**: Panel de configuraci√≥n en frontend
- ‚úÖ **Authentication**: API protegida con API Key validation

---

## üèóÔ∏è Arquitectura T√©cnica

### Backend Components

#### 1. LLM Provider Manager (`api/services/llm_provider.py`)

**Clase Principal: `LLMProviderManager`**

```python
class LLMProviderManager:
    """
    Orquestador central de proveedores LLM con fallback autom√°tico
    """
    def __init__(self):
        self.active_provider = "llm_studio"  # Default
        self.providers = {
            "llm_studio": {...},  # OpenAI-compatible API
            "phi4_local": Phi4Local(),  # Pattern-based local
            "offline": OfflineLLM()  # Rules-based engine
        }
        self.statistics = {...}  # Per-provider stats
    
    async def generate(self, prompt: str, context: Dict) -> Dict:
        """
        Main entry point con fallback autom√°tico:
        1. Intenta con provider activo
        2. Si falla (timeout/error), intenta siguiente en cadena
        3. Retorna respuesta con metadata (provider usado, latencia)
        """
```

**Proveedores Implementados:**

1. **LLM Studio (Primary)**
   - URL: `http://100.101.115.5:2714/v1/completions`
   - Formato: OpenAI-compatible
   - Modelo: Phi-4
   - Timeout: 40 segundos
   - Autenticaci√≥n: Bearer token (opcional)

2. **Phi4Local (Fallback)**
   - Ejecuci√≥n: Local (CPU/RAM)
   - M√©todo: Pattern matching para clasificaci√≥n de severidad
   - Respuesta: < 1 segundo
   - Patrones: "critical", "high", "medium", "low" keywords

3. **OfflineLLM (Emergency)**
   - Sin AI: Rules-based engine
   - Evaluaci√≥n: Conteo de hallazgos cr√≠ticos
   - Siempre disponible
   - Respuesta instant√°nea

#### 2. API Router (`api/routes/llm_settings.py`)

**Endpoints Implementados:**

| Endpoint | M√©todo | Descripci√≥n | Auth |
|----------|--------|-------------|------|
| `/api/v41/llm/status` | GET | Estado completo del sistema LLM | ‚úì |
| `/api/v41/llm/provider` | POST | Cambiar proveedor activo | ‚úì |
| `/api/v41/llm/test` | POST | Test con prompt personalizado | ‚úì |
| `/api/v41/llm/health` | GET | Health check de todos los proveedores | ‚úì |
| `/api/v41/llm/statistics` | GET | M√©tricas de uso por proveedor | ‚úì |
| `/api/v41/llm/analyze` | POST | An√°lisis SOAR con LLM | ‚úì |
| `/api/v41/llm/models` | GET | Lista de modelos disponibles | ‚úì |
| `/api/v41/llm/reset-stats` | POST | Reiniciar estad√≠sticas | ‚úì |

**Ejemplo Request:**

```bash
curl -X POST http://localhost:8080/api/v41/llm/test \
  -H "X-API-Key: your-api-key" \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "Analiza estos hallazgos forenses y clasifica severidad",
    "context": {
      "findings": ["Lateral movement detected", "Malicious PowerShell"],
      "timestamp": "2024-12-07T13:00:00Z"
    }
  }'
```

**Ejemplo Response:**

```json
{
  "response": {
    "provider": "llm_studio",
    "output": "An√°lisis: Severidad CR√çTICA. Movimiento lateral indica...",
    "latency": 2.35,
    "model": "phi-4"
  }
}
```

#### 3. SOAR Intelligence Integration (`api/services/soar_intelligence.py`)

**Actualizaci√≥n v4.3:**

```python
# ANTES (v1.0):
from api.services.llm_local import generate_local

# AHORA (v4.3):
from api.services.llm_provider import llm_manager

# Uso en an√°lisis:
result = await llm_manager.generate(
    prompt=f"Analiza estos hallazgos: {findings}",
    context={"case_id": case_id, "severity": "high"}
)
```

**Capacidades SOAR + LLM:**

- üîç **Severity Classification**: Clasifica hallazgos autom√°ticamente
- üìã **Action Recommendation**: Sugiere acciones de respuesta
- üéØ **IOC Extraction**: Extrae indicadores de compromiso
- üß† **Threat Intelligence**: Enriquecimiento contextual
- üìä **Correlation Analysis**: Correlaciona eventos relacionados

---

### Frontend Components

#### 1. React Panel (`frontend-react/src/components/Settings/LLMSettings.jsx`)

**Caracter√≠sticas:**

- **Estado del Sistema**: Display de proveedor activo y salud global
- **Cambio de Proveedor**: Botones para switch manual entre proveedores
- **Estad√≠sticas de Uso**: Requests, errores, latencia por proveedor
- **Test de Modelo**: Interfaz para probar prompts personalizados
- **Health Monitoring**: Estado de cada proveedor en tiempo real
- **Configuraci√≥n**: Visualizaci√≥n de par√°metros de cada modelo

**Screenshot:**

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ üß† Configuraci√≥n de IA (LLM)                       ‚îÇ
‚îÇ Gesti√≥n de modelos, proveedores y configuraci√≥n    ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                     ‚îÇ
‚îÇ üìä Estado del Sistema                              ‚îÇ
‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ ‚îÇ Proveedor   ‚îÇ ‚îÇ Estado Global‚îÇ ‚îÇ Total Reqs   ‚îÇ ‚îÇ
‚îÇ ‚îÇ ‚òÅÔ∏è LLM     ‚îÇ ‚îÇ üü¢ Saludable ‚îÇ ‚îÇ 1,234        ‚îÇ ‚îÇ
‚îÇ ‚îÇ   Studio    ‚îÇ ‚îÇ              ‚îÇ ‚îÇ              ‚îÇ ‚îÇ
‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îÇ                                                     ‚îÇ
‚îÇ üîÑ Proveedores LLM                                 ‚îÇ
‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ ‚îÇ ‚òÅÔ∏è LLM Studio                      [ACTIVO]  ‚îÇ  ‚îÇ
‚îÇ ‚îÇ Jeturing AI Platform - Phi-4                 ‚îÇ  ‚îÇ
‚îÇ ‚îÇ http://100.101.115.5:2714        ‚úì Disponible‚îÇ  ‚îÇ
‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ ‚îÇ üíª Phi-4 Local                               ‚îÇ  ‚îÇ
‚îÇ ‚îÇ Pattern-based local model (fallback)         ‚îÇ  ‚îÇ
‚îÇ ‚îÇ                                  ‚úì Disponible‚îÇ  ‚îÇ
‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ ‚îÇ üìã Offline Engine                            ‚îÇ  ‚îÇ
‚îÇ ‚îÇ Rules-based engine (no AI required)          ‚îÇ  ‚îÇ
‚îÇ ‚îÇ                                  ‚úì Disponible‚îÇ  ‚îÇ
‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ                                                     ‚îÇ
‚îÇ üí° Fallback autom√°tico: Si LLM Studio no responde, ‚îÇ
‚îÇ    el sistema cambiar√° autom√°ticamente a Phi-4     ‚îÇ
‚îÇ                                                     ‚îÇ
‚îÇ üß™ Test de Modelo LLM                              ‚îÇ
‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ ‚îÇ Prompt: ________________________________     ‚îÇ  ‚îÇ
‚îÇ ‚îÇ         ________________________________     ‚îÇ  ‚îÇ
‚îÇ ‚îÇ         ________________________________     ‚îÇ  ‚îÇ
‚îÇ ‚îÇ [üöÄ Ejecutar Test]                           ‚îÇ  ‚îÇ
‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Acceso:**
- URL: `/settings/llm`
- Men√∫: Settings ‚Üí LLM Configuration
- Permisos: Requiere autenticaci√≥n de usuario

---

## üîß Configuraci√≥n

### Variables de Entorno (`.env.local`)

```bash
# ============================================
# LLM STUDIO INTEGRATION (v4.3)
# ============================================

# Proveedor activo: llm_studio, phi4_local, offline
LLM_PROVIDER=llm_studio

# LLM Studio (Jeturing AI Platform)
LLM_STUDIO_URL=http://100.101.115.5:2714/v1/completions
LLM_STUDIO_API_KEY=
LLM_STUDIO_MODEL=phi-4
LLM_STUDIO_TIMEOUT=40

# Phi-4 Local (Fallback)
PHI4_LOCAL_ENABLED=true

# Offline Mode (Emergency fallback)
OFFLINE_LLM_ENABLED=true
```

### Backend Settings (`api/config.py`)

```python
class Settings(BaseSettings):
    # ...existing settings...
    
    # ============================================================================
    # LLM STUDIO INTEGRATION (v4.3)
    # ============================================================================
    LLM_PROVIDER: str = "llm_studio"
    LLM_STUDIO_URL: str = "http://100.101.115.5:2714/v1/completions"
    LLM_STUDIO_API_KEY: Optional[str] = None
    LLM_STUDIO_MODEL: str = "phi-4"
    LLM_STUDIO_TIMEOUT: int = 40
    PHI4_LOCAL_ENABLED: bool = True
    OFFLINE_LLM_ENABLED: bool = True
```

---

## üöÄ Uso y Ejemplos

### 1. Verificar Estado del Sistema

```bash
curl -X GET http://localhost:8080/api/v41/llm/status \
  -H "X-API-Key: your-api-key"
```

**Response:**

```json
{
  "active_provider": "llm_studio",
  "available_providers": [
    {
      "name": "llm_studio",
      "description": "LLM Studio - Jeturing AI Platform (Phi-4)",
      "status": "available",
      "url": "http://100.101.115.5:2714/v1/completions"
    },
    {
      "name": "phi4_local",
      "description": "Phi-4 Local - Pattern-based analysis",
      "status": "available"
    },
    {
      "name": "offline",
      "description": "Offline - Rules-based engine",
      "status": "available"
    }
  ],
  "configuration": {
    "llm_studio": {
      "model": "phi-4",
      "timeout": 40,
      "url": "http://100.101.115.5:2714/v1/completions"
    }
  }
}
```

### 2. Cambiar Proveedor Activo

```bash
curl -X POST http://localhost:8080/api/v41/llm/provider \
  -H "X-API-Key: your-api-key" \
  -H "Content-Type: application/json" \
  -d '{
    "provider": "phi4_local",
    "reason": "LLM Studio no disponible temporalmente"
  }'
```

### 3. Test de Modelo con Prompt Personalizado

```bash
curl -X POST http://localhost:8080/api/v41/llm/test \
  -H "X-API-Key: your-api-key" \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "Analiza estos eventos de seguridad y determina la severidad:\n- M√∫ltiples intentos de login fallidos\n- PowerShell execution con flags ofuscados\n- Conexi√≥n a IP conocida como C2",
    "context": {
      "case_id": "IR-2025-001",
      "timestamp": "2024-12-07T13:00:00Z"
    }
  }'
```

**Response:**

```json
{
  "response": {
    "provider": "llm_studio",
    "output": "AN√ÅLISIS DE SEVERIDAD: CR√çTICA\n\nHallazgos:\n1. Login fallidos m√∫ltiples ‚Üí Posible ataque de fuerza bruta\n2. PowerShell ofuscado ‚Üí Alta probabilidad de malware\n3. Conexi√≥n a C2 conocido ‚Üí Confirmaci√≥n de compromiso\n\nRECOMENDACIONES:\n- Aislar sistema inmediatamente\n- Capturar memoria y disco para an√°lisis forense\n- Revisar lateral movement en red\n- Verificar persistencia en sistema",
    "latency": 3.45,
    "model": "phi-4",
    "timestamp": "2024-12-07T13:05:23Z"
  }
}
```

### 4. An√°lisis SOAR con LLM

```bash
curl -X POST http://localhost:8080/api/v41/llm/analyze \
  -H "X-API-Key: your-api-key" \
  -H "Content-Type: application/json" \
  -d '{
    "case_id": "IR-2025-001",
    "evidence": {
      "type": "endpoint_scan",
      "findings": [
        "Suspicious process: cmd.exe spawned from word.exe",
        "Network connection to 192.168.1.50:4444",
        "Registry persistence: HKCU\\Software\\Microsoft\\Windows\\CurrentVersion\\Run"
      ]
    }
  }'
```

**Response:**

```json
{
  "analysis": {
    "severity": "CRITICAL",
    "confidence": 0.95,
    "iocs": [
      {
        "type": "process",
        "value": "cmd.exe",
        "context": "Spawned from word.exe (suspicious)"
      },
      {
        "type": "ip",
        "value": "192.168.1.50:4444",
        "context": "Potential C2 communication"
      },
      {
        "type": "registry",
        "value": "HKCU\\Software\\Microsoft\\Windows\\CurrentVersion\\Run",
        "context": "Persistence mechanism"
      }
    ],
    "recommended_actions": [
      "Isolate affected endpoint immediately",
      "Block network communication to 192.168.1.50",
      "Capture memory dump for analysis",
      "Check for lateral movement",
      "Review email attachments for initial compromise vector"
    ],
    "provider_used": "llm_studio",
    "processing_time": 2.89
  }
}
```

### 5. Health Check

```bash
curl -X GET http://localhost:8080/api/v41/llm/health \
  -H "X-API-Key: your-api-key"
```

**Response:**

```json
{
  "health": {
    "overall": "healthy",
    "providers": {
      "llm_studio": {
        "status": "healthy",
        "response_time": 1.23
      },
      "phi4_local": {
        "status": "healthy",
        "response_time": 0.05
      },
      "offline": {
        "status": "healthy",
        "response_time": 0.001
      }
    },
    "timestamp": "2024-12-07T13:10:00Z"
  }
}
```

### 6. Estad√≠sticas de Uso

```bash
curl -X GET http://localhost:8080/api/v41/llm/statistics \
  -H "X-API-Key: your-api-key"
```

**Response:**

```json
{
  "llm_statistics": {
    "llm_studio": {
      "requests": 1234,
      "errors": 3,
      "avg_latency": 2.45
    },
    "phi4_local": {
      "requests": 45,
      "errors": 0,
      "avg_latency": 0.12
    },
    "offline": {
      "requests": 2,
      "errors": 0,
      "avg_latency": 0.001
    }
  },
  "timestamp": "2024-12-07T13:15:00Z"
}
```

---

## üìä Flujo de Fallback Autom√°tico

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Usuario/Sistema solicita an√°lisis LLM             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                    ‚îÇ
                    ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  LLMProviderManager.generate()                     ‚îÇ
‚îÇ  Provider activo: llm_studio                       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                    ‚îÇ
                    ‚ñº
       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
       ‚îÇ  Intenta con LLM Studio    ‚îÇ
       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                    ‚îÇ
         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
         ‚îÇ                     ‚îÇ
    ‚úÖ Success            ‚ùå Timeout/Error
         ‚îÇ                     ‚îÇ
         ‚îÇ                     ‚ñº
         ‚îÇ         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
         ‚îÇ         ‚îÇ Intenta Phi-4 Local    ‚îÇ
         ‚îÇ         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ                    ‚îÇ
         ‚îÇ         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
         ‚îÇ         ‚îÇ                     ‚îÇ
         ‚îÇ    ‚úÖ Success            ‚ùå Error
         ‚îÇ         ‚îÇ                     ‚îÇ
         ‚îÇ         ‚îÇ                     ‚ñº
         ‚îÇ         ‚îÇ         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
         ‚îÇ         ‚îÇ         ‚îÇ Intenta Offline    ‚îÇ
         ‚îÇ         ‚îÇ         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ         ‚îÇ                    ‚îÇ
         ‚îÇ         ‚îÇ                ‚úÖ Always works
         ‚îÇ         ‚îÇ                    ‚îÇ
         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                   ‚îÇ
                   ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Retorna respuesta con metadata:                   ‚îÇ
‚îÇ  - provider_used: "llm_studio" | "phi4_local" ...  ‚îÇ
‚îÇ  - latency: 2.45                                   ‚îÇ
‚îÇ  - output: "An√°lisis completo..."                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Logging durante Fallback:**

```
[INFO] üß† LLMProviderManager: Attempting generation with llm_studio
[ERROR] ‚ùå llm_studio failed: Timeout after 40s
[INFO] üîÑ Falling back to phi4_local
[INFO] ‚úÖ phi4_local succeeded in 0.12s
```

---

## üîí Seguridad

### Autenticaci√≥n API

Todos los endpoints LLM requieren API Key validation:

```python
@router.get("/status")
async def get_status(api_key: str = Depends(verify_api_key)):
    # Endpoint protegido
```

### Variables Sensibles

- **LLM_STUDIO_API_KEY**: Nunca hardcodeada, siempre desde variables de entorno
- **API_KEY**: Requerida en headers para acceso a endpoints
- **Bearer tokens**: Manejados por llm_provider con logging sanitizado

### Rate Limiting

**Implementado a nivel de provider:**

```python
# En llm_provider.py
if time.time() - self.statistics["llm_studio"]["last_request"] < 1.0:
    await asyncio.sleep(1.0)  # Throttle requests
```

---

## üß™ Testing

### Test Manual con curl

```bash
# 1. Verificar salud
curl -X GET http://localhost:8080/api/v41/llm/health \
  -H "X-API-Key: your-key"

# 2. Test de modelo
curl -X POST http://localhost:8080/api/v41/llm/test \
  -H "X-API-Key: your-key" \
  -H "Content-Type: application/json" \
  -d '{"prompt": "Test prompt", "context": {}}'

# 3. Cambiar a fallback
curl -X POST http://localhost:8080/api/v41/llm/provider \
  -H "X-API-Key: your-key" \
  -H "Content-Type: application/json" \
  -d '{"provider": "phi4_local", "reason": "Testing fallback"}'
```

### Test Autom√°tico (Python)

```python
import requests

API_URL = "http://localhost:8080/api/v41/llm"
API_KEY = "your-api-key"
HEADERS = {"X-API-Key": API_KEY, "Content-Type": "application/json"}

# Test 1: Status check
response = requests.get(f"{API_URL}/status", headers=HEADERS)
assert response.status_code == 200
status = response.json()
assert status["active_provider"] in ["llm_studio", "phi4_local", "offline"]

# Test 2: Generate with prompt
test_prompt = {
    "prompt": "Classify severity: malicious PowerShell detected",
    "context": {"case_id": "TEST-001"}
}
response = requests.post(f"{API_URL}/test", headers=HEADERS, json=test_prompt)
assert response.status_code == 200
result = response.json()
assert "response" in result
assert "provider" in result["response"]
assert "output" in result["response"]

# Test 3: Fallback behavior
# Simular timeout cambiando a offline
requests.post(
    f"{API_URL}/provider",
    headers=HEADERS,
    json={"provider": "offline", "reason": "Test fallback"}
)
response = requests.post(f"{API_URL}/test", headers=HEADERS, json=test_prompt)
result = response.json()
assert result["response"]["provider"] == "offline"

print("‚úÖ All tests passed")
```

---

## üìà Monitoring y Observabilidad

### Logs Estructurados

```python
# Formato de logs
[2024-12-07 13:00:00] [INFO] üß† LLMProviderManager: generate() called with provider=llm_studio
[2024-12-07 13:00:02] [INFO] ‚úÖ llm_studio response received (latency=2.35s)
[2024-12-07 13:00:02] [INFO] üìä Statistics updated: requests=1235, errors=3
```

### M√©tricas Disponibles

**Por Provider:**
- `requests`: Total de requests procesados
- `errors`: Total de errores/timeouts
- `avg_latency`: Latencia promedio en segundos
- `last_request`: Timestamp de √∫ltimo request

**Acceso:**
```bash
curl http://localhost:8080/api/v41/llm/statistics -H "X-API-Key: key"
```

### Alertas Recomendadas

**Configurar alertas cuando:**
- Error rate > 10% en 5 minutos ‚Üí Revisar conectividad a LLM Studio
- Latencia > 10s promedio ‚Üí Considerar aumentar timeout
- Fallback usage > 50% ‚Üí LLM Studio podr√≠a estar degradado

---

## üîÑ Actualizaciones Futuras (Roadmap)

### v4.4 (Planificado)

- [ ] **Model Switching**: Cambiar modelo sin cambiar proveedor
- [ ] **Fine-tuning Integration**: Usar modelos fine-tuned espec√≠ficos de forense
- [ ] **Prompt Templates**: Biblioteca de prompts optimizados
- [ ] **Batch Processing**: Procesar m√∫ltiples an√°lisis en batch
- [ ] **WebSocket Updates**: Push notifications de an√°lisis completados

### v4.5 (Futuro)

- [ ] **Multi-Model Consensus**: Usar m√∫ltiples modelos y combinar resultados
- [ ] **Cost Tracking**: Monitoreo de costos por provider (si aplicable)
- [ ] **A/B Testing**: Comparar outputs de diferentes modelos
- [ ] **Custom Providers**: API para registrar proveedores externos

---

## üêõ Troubleshooting

### Problema: LLM Studio no responde

**S√≠ntomas:**
```
[ERROR] ‚ùå llm_studio failed: Timeout after 40s
[INFO] üîÑ Falling back to phi4_local
```

**Soluci√≥n:**
1. Verificar conectividad: `curl http://100.101.115.5:2714/health`
2. Revisar logs de LLM Studio en servidor
3. Aumentar timeout: `LLM_STUDIO_TIMEOUT=60` en `.env.local`
4. Confirmar que modelo Phi-4 est√° cargado en servidor

### Problema: Phi-4 Local no funciona

**S√≠ntomas:**
```
[ERROR] ‚ùå phi4_local failed: Pattern matching error
```

**Soluci√≥n:**
1. Verificar que `PHI4_LOCAL_ENABLED=true` en `.env.local`
2. Revisar implementaci√≥n de `Phi4Local.generate()` en `llm_provider.py`
3. Validar que el prompt contiene keywords esperados

### Problema: Frontend no muestra panel LLM

**S√≠ntomas:**
- URL `/settings/llm` muestra error 404
- Panel no aparece en men√∫ Settings

**Soluci√≥n:**
1. Verificar import en `App.jsx`: `import LLMSettings from './components/Settings/LLMSettings'`
2. Confirmar ruta registrada: `<Route path="settings/llm" element={<LLMSettings />} />`
3. Verificar que componente existe: `ls frontend-react/src/components/Settings/LLMSettings.jsx`
4. Rebuild frontend: `cd frontend-react && npm run build`

### Problema: API devuelve 401 Unauthorized

**S√≠ntomas:**
```json
{"detail": "Invalid API Key"}
```

**Soluci√≥n:**
1. Verificar que header incluye: `X-API-Key: your-key`
2. Confirmar API key v√°lida en backend: `api/config.py` ‚Üí `API_KEY`
3. Revisar middleware: `api/middleware/auth.py` ‚Üí `verify_api_key()`

---

## üìö Referencias

### Documentaci√≥n Relacionada

- [SOAR Intelligence Engine](./SOAR_INTELLIGENCE.md)
- [API Reference](./API.md)
- [Configuration Guide](../installation/CONFIGURATION.md)
- [Frontend Development](../frontend/DEVELOPMENT.md)

### Enlaces Externos

- [LLM Studio Documentation](http://100.101.115.5:2714/docs)
- [Phi-4 Model Card](https://huggingface.co/microsoft/phi-4)
- [OpenAI API Format](https://platform.openai.com/docs/api-reference)

### C√≥digo Fuente

- Backend Provider: `api/services/llm_provider.py`
- Backend Router: `api/routes/llm_settings.py`
- Frontend Panel: `frontend-react/src/components/Settings/LLMSettings.jsx`
- Configuration: `api/config.py`
- Environment: `.env.local`

---

**√öltima actualizaci√≥n**: 7 de diciembre de 2024  
**Mantenedor**: MCP Kali Forensics Team  
**Versi√≥n**: v4.3  
**Estado**: ‚úÖ Producci√≥n Ready

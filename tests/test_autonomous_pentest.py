"""
Tests para módulo Autonomous Pentesting v4.5.0
Cobertura: planner LLM, executor con sandbox, approval gate, report generation
"""

import pytest
from unittest.mock import Mock, patch, AsyncMock, MagicMock
from datetime import datetime
from fastapi.testclient import TestClient
import json

from api.routes import pentest
from api.services.autonomous_pentest import (
    plan_pentest_with_llm,
    execute_pentest_plan,
    validate_and_correlate_findings,
    generate_pentest_report,
    _validate_plan_against_policy,
    _parse_tool_output,
)
from api.services.llm_integration import safe_llm_call


class TestPentestPlanner:
    """Tests para LLM Planner Agent"""
    
    @pytest.mark.asyncio
    @patch('api.services.autonomous_pentest.safe_llm_call')
    @patch('api.services.autonomous_pentest._load_pentest_policy')
    @patch('api.services.autonomous_pentest._load_case_context')
    async def test_plan_generation_respects_policy(
        self,
        mock_load_context,
        mock_load_policy,
        mock_llm_call
    ):
        """Verificar que plan respeta scope y acciones permitidas"""
        
        # Setup
        mock_load_policy.return_value = {
            "policy_id": "PTP-001",
            "allowed_actions": ["recon", "vuln_scan"],
            "forbidden_actions": ["exploit", "dos"],
            "scope": {"domains": ["test.com"]}
        }
        
        mock_load_context.return_value = {
            "case_id": "IR-001",
            "discovered_hosts": []
        }
        
        # Respuesta del LLM con plan válido
        mock_llm_call.return_value = """{
            "tasks": [
                {
                    "task_id": "T-001",
                    "agent": "recon",
                    "tool": "nmap",
                    "reason": "Identify services",
                    "risk_level": "LOW",
                    "requires_approval": false
                }
            ],
            "escalation_required": false
        }"""
        
        # Ejecutar
        plan = await plan_pentest_with_llm(
            pentest_id="PT-001",
            case_id="IR-001",
            policy_id="PTP-001",
            objectives="Basic assessment"
        )
        
        # Verificar
        assert plan["tasks"][0]["agent"] == "recon"
        assert len(plan["tasks"]) > 0
    
    @pytest.mark.asyncio
    @patch('api.services.autonomous_pentest.safe_llm_call')
    @patch('api.services.autonomous_pentest._load_pentest_policy')
    @patch('api.services.autonomous_pentest._load_case_context')
    async def test_plan_rejects_forbidden_actions(
        self,
        mock_load_context,
        mock_load_policy,
        mock_llm_call
    ):
        """Verificar que plan rechaza acciones prohibidas"""
        
        mock_load_policy.return_value = {
            "forbidden_actions": ["exploit"]
        }
        
        mock_load_context.return_value = {}
        
        # LLM retorna plan con acción prohibida
        mock_llm_call.return_value = """{
            "tasks": [
                {"agent": "exploit", "risk_level": "HIGH"}
            ]
        }"""
        
        # Debería lanzar excepción
        with pytest.raises(ValueError, match="forbidden"):
            await plan_pentest_with_llm(
                pentest_id="PT-001",
                case_id="IR-001",
                policy_id="PTP-001"
            )


class TestExecutorSandbox:
    """Tests para Executor con sandboxing"""
    
    @pytest.mark.asyncio
    @patch('api.services.executor_engine.ToolExecutor.execute_sandboxed')
    async def test_execute_task_with_timeout(self, mock_sandbox_exec):
        """Verificar que executor respeta timeouts"""
        
        from api.services.autonomous_pentest import execute_pentest_plan
        
        mock_sandbox_exec.return_value = "nmap output..."
        
        action_plan = {
            "tasks": [
                {
                    "task_id": "T-001",
                    "agent": "recon",
                    "tool": "nmap",
                    "command": "nmap localhost",
                    "risk_level": "LOW",
                    "requires_approval": False
                }
            ]
        }
        
        results = await execute_pentest_plan(
            pentest_id="PT-001",
            case_id="IR-001",
            action_plan=action_plan,
            policy={}
        )
        
        assert len(results["completed_tasks"]) > 0
        assert results["success_rate"] > 0
    
    @pytest.mark.asyncio
    @patch('api.services.executor_engine.ToolExecutor.execute_sandboxed')
    async def test_timeout_handling(self, mock_sandbox_exec):
        """Verificar manejo de timeouts"""
        
        from api.services.autonomous_pentest import execute_pentest_plan
        
        mock_sandbox_exec.side_effect = asyncio.TimeoutError()
        
        action_plan = {
            "tasks": [{"task_id": "T-001", "agent": "recon", "command": "sleep 600"}]
        }
        
        results = await execute_pentest_plan(
            pentest_id="PT-001",
            case_id="IR-001",
            action_plan=action_plan,
            policy={}
        )
        
        assert len(results["failed_tasks"]) > 0
        assert "TIMEOUT" in [t["error"] for t in results["failed_tasks"]]


class TestApprovalGate:
    """Tests para Human Approval Gate"""
    
    @pytest.mark.asyncio
    @patch('api.services.autonomous_pentest._request_human_approval')
    @patch('api.services.executor_engine.ToolExecutor.execute_sandboxed')
    async def test_medium_risk_requires_approval(
        self,
        mock_sandbox_exec,
        mock_approval
    ):
        """Verificar que acciones MEDIUM requieren aprobación"""
        
        from api.services.autonomous_pentest import execute_pentest_plan
        
        mock_approval.return_value = {"approved": True}
        mock_sandbox_exec.return_value = "output"
        
        action_plan = {
            "tasks": [
                {
                    "task_id": "T-002",
                    "agent": "web",
                    "tool": "nuclei",
                    "risk_level": "MEDIUM",
                    "requires_approval": True,
                    "command": "nuclei -u http://test"
                }
            ]
        }
        
        results = await execute_pentest_plan(
            pentest_id="PT-001",
            case_id="IR-001",
            action_plan=action_plan,
            policy={}
        )
        
        # Verificar que se pidió aprobación
        mock_approval.assert_called()


class TestLLMIntegration:
    """Tests para integración segura con LLM"""
    
    @pytest.mark.asyncio
    @patch('api.services.llm_integration.httpx.AsyncClient.post')
    async def test_safe_llm_call_with_timeout(self, mock_post):
        """Verificar que safe_llm_call respeta timeout"""
        
        mock_response = AsyncMock()
        mock_response.json.return_value = {
            "choices": [{"text": "Plan: ..."}]
        }
        mock_post.return_value.__aenter__.return_value = mock_response
        
        response = await safe_llm_call(
            prompt="Test prompt",
            timeout=30,
            provider_hint="local"
        )
        
        assert len(response) > 0
    
    def test_sanitize_rejects_pii(self):
        """Verificar que sanitización rechaza PII"""
        
        from api.services.llm_integration import _validate_and_sanitize_prompt
        
        # Debe rechazar password
        with pytest.raises(ValueError, match="forbidden"):
            _validate_and_sanitize_prompt("password: supersecret123")
        
        # Debe rechazar API key
        with pytest.raises(ValueError, match="forbidden"):
            _validate_and_sanitize_prompt("api_key=secret-token-xyz")
    
    def test_sanitize_accepts_clean_prompt(self):
        """Verificar que sanitización acepta prompts limpios"""
        
        from api.services.llm_integration import _validate_and_sanitize_prompt
        
        # Debe aceptar prompt limpio
        _validate_and_sanitize_prompt("Plan a penetration test for empresa.com")
        
        # No debe lanzar excepción


class TestToolOutputParsing:
    """Tests para parsing de output de herramientas"""
    
    def test_parse_nmap_output(self):
        """Verificar parsing de nmap"""
        
        nmap_output = """
        22/tcp   open  ssh
        80/tcp   open  http
        443/tcp  open  https
        """
        
        result = _parse_tool_output("nmap", nmap_output)
        
        assert result["tool"] == "nmap"
        assert result["open_ports"] > 0
    
    def test_parse_nuclei_output(self):
        """Verificar parsing de nuclei"""
        
        nuclei_output = """
        {"template": "http-brute-force", "severity": "high"}
        {"template": "ssl-cert-check", "severity": "medium"}
        """
        
        result = _parse_tool_output("nuclei", nuclei_output)
        
        assert result["tool"] == "nuclei"
        assert result["findings_count"] == 2


class TestPentestRoutes:
    """Tests para endpoints REST"""
    
    def test_start_pentest_endpoint(self):
        """Verificar endpoint POST /pentest/start"""
        
        from api.main import app
        
        client = TestClient(app)
        
        payload = {
            "case_id": "IR-2025-001",
            "policy_id": "PTP-2025-001",
            "objectives": "Identify web vulnerabilities"
        }
        
        # Mock verify_api_key
        with patch('api.middleware.auth.verify_api_key'):
            response = client.post(
                "/pentest/start",
                json=payload,
                headers={"X-API-Key": "test-key"}
            )
        
        assert response.status_code == 200
        data = response.json()
        assert "pentest_id" in data
        assert data["status"] == "queued"
    
    def test_get_pentest_status_endpoint(self):
        """Verificar endpoint GET /pentest/{id}/status"""
        
        from api.main import app
        from api.services.autonomous_pentest import _store_pentest_state
        
        client = TestClient(app)
        
        # Crear estado inicial
        pentest_id = "PT-001"
        _store_pentest_state(pentest_id, {
            "pentest_id": pentest_id,
            "status": "planning",
            "progress_percentage": 10,
            "current_action": "Starting planner...",
            "created_at": "2025-12-15T10:00:00Z",
            "updated_at": "2025-12-15T10:00:00Z",
        })
        
        response = client.get(f"/pentest/{pentest_id}/status")
        
        assert response.status_code == 200
        data = response.json()
        assert data["pentest_id"] == pentest_id
        assert data["status"] in ["planning", "awaiting_approval", "executing", "completed", "failed", "cancelled"]


class TestReportGeneration:
    """Tests para generación de reportes"""
    
    @pytest.mark.asyncio
    async def test_generate_pentest_report(self, tmp_path):
        """Verificar generación de reporte"""
        
        # Mock EVIDENCE_DIR
        with patch('api.services.autonomous_pentest.settings.EVIDENCE_DIR', tmp_path):
            report_path = await generate_pentest_report(
                pentest_id="PT-001",
                case_id="IR-001",
                execution_results={
                    "total_tasks": 5,
                    "completed_tasks": [{"task_id": "T-1"}] * 4,
                    "failed_tasks": [{"task_id": "T-5"}],
                    "evidence_path": "/tmp/evidence",
                    "success_rate": 80
                },
                validated_findings={"validated_findings": []}
            )
        
        # Verificar que archivo fue creado
        assert "report.md" in report_path


# ============================================================================
# PYTEST MARKERS & FIXTURES
# ============================================================================

import asyncio

@pytest.fixture
def event_loop():
    """Fixture para asyncio event loop"""
    loop = asyncio.get_event_loop_policy().new_event_loop()
    yield loop
    loop.close()


if __name__ == "__main__":
    pytest.main([__file__, "-v"])

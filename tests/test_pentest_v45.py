"""
Tests for Autonomous Pentesting (Governed) v4.5.0
Validates compliance with docs/agents/RED_AGENT.md and v4.5.0 architecture.
Includes Unit Tests and API Integration Tests.
"""

import pytest
import asyncio
import json
from unittest.mock import AsyncMock, MagicMock, patch
from datetime import datetime
from pathlib import Path
from fastapi.testclient import TestClient
from fastapi import FastAPI

from api.models.pentest import (
    PentestRequest, PentestPlan, PentestTask, PentestPolicy, 
    PentestRisk, PentestPhase, ApprovalRequest, PentestTaskResult
)
from api.services.pentest_planner import generate_pentest_plan
from api.services.pentest_validator import validate_plan
from api.services.pentest_executor import PentestExecutor
from api.services.pentest_policy import get_active_policy
from api.routes.pentest import router as pentest_router

# ============================================================================
# MOCKS & FIXTURES
# ============================================================================

@pytest.fixture
def mock_policy():
    return PentestPolicy(
        policy_id="TEST-POLICY",
        case_id="TEST-CASE",
        name="Test Policy",
        intensity="SAFE",
        allowed_actions=["recon", "vuln_scan"],
        forbidden_actions=["exploit", "dos"],
        approval_matrix={"LOW": False, "MEDIUM": True, "HIGH": True, "OFFENSIVE": True},
        human_approval_required=True
    )

@pytest.fixture
def mock_llm_response():
    return """
    {
        "tasks": [
            {
                "task_id": "TASK-001",
                "agent": "red.recon",
                "tool": "nmap",
                "command": ["nmap", "-sV", "target.com"],
                "reason": "Recon",
                "phase": "recon",
                "risk_level": "LOW",
                "requires_approval": false,
                "sandbox": true,
                "timeout": 300
            },
            {
                "task_id": "TASK-002",
                "agent": "red.vuln",
                "tool": "nikto",
                "command": ["nikto", "-h", "target.com"],
                "reason": "Vuln scan",
                "phase": "web",
                "risk_level": "MEDIUM",
                "requires_approval": false,
                "sandbox": true,
                "timeout": 600
            }
        ],
        "escalation_required": false
    }
    """

@pytest.fixture
def test_client():
    """Creates a TestClient for the pentest router."""
    app = FastAPI()
    app.include_router(pentest_router)
    return TestClient(app)

# ============================================================================
# UNIT TESTS (Services)
# ============================================================================

@pytest.mark.asyncio
async def test_planner_generates_valid_plan(mock_policy, mock_llm_response):
    """
    Validates that the Planner service correctly parses LLM output into a PentestPlan.
    """
    request = PentestRequest(case_id="TEST-CASE", targets={"domains": ["target.com"]})
    
    with patch("api.services.pentest_planner.safe_llm_call", new_callable=AsyncMock) as mock_llm:
        mock_llm.return_value = mock_llm_response
        
        plan = await generate_pentest_plan(request, mock_policy)
        
        assert isinstance(plan, PentestPlan)
        assert len(plan.tasks) == 2
        assert plan.tasks[0].tool == "nmap"
        assert plan.tasks[1].risk_level == PentestRisk.MEDIUM

@pytest.mark.asyncio
async def test_validator_enforces_policy(mock_policy):
    """
    Validates that the Validator service enforces policy constraints:
    - Flags MEDIUM risk tasks for approval (per mock_policy)
    - Blocks forbidden tools
    """
    tasks = [
        PentestTask(
            task_id="T1", agent="recon", tool="nmap", command=[], 
            risk_level=PentestRisk.LOW, requires_approval=False
        ),
        PentestTask(
            task_id="T2", agent="vuln", tool="nikto", command=[], 
            risk_level=PentestRisk.MEDIUM, requires_approval=False
        ),
        PentestTask(
            task_id="T3", agent="exploit", tool="exploit", command=[], 
            risk_level=PentestRisk.HIGH, requires_approval=False
        )
    ]
    plan = PentestPlan(pentest_id="PT-1", tasks=tasks)
    
    # Update policy to forbid 'exploit' tool explicitly for this test
    mock_policy.forbidden_actions = ["exploit"]
    
    validated_plan = validate_plan(plan, mock_policy)
    
    # T1 (LOW) -> Should pass without approval
    t1 = next(t for t in validated_plan.tasks if t.task_id == "T1")
    assert t1.requires_approval is False
    
    # T2 (MEDIUM) -> Should require approval (policy matrix says MEDIUM=True)
    t2 = next(t for t in validated_plan.tasks if t.task_id == "T2")
    assert t2.requires_approval is True
    
    # T3 (exploit) -> Should be removed/filtered out
    t3_exists = any(t.task_id == "T3" for t in validated_plan.tasks)
    assert not t3_exists

@pytest.mark.asyncio
async def test_executor_approval_gate():
    """
    Validates that the Executor pauses on tasks requiring approval and resumes after approval.
    """
    executor = PentestExecutor()
    
    # Mock ToolExecutor to avoid actual execution
    executor.executor.execute_sandboxed = AsyncMock(return_value={"success": True, "output": "OK"})
    
    # Create a plan with one task requiring approval
    task = PentestTask(
        task_id="T-GATE", agent="test", tool="echo", command=["echo", "hello"],
        risk_level=PentestRisk.MEDIUM, requires_approval=True
    )
    plan = PentestPlan(pentest_id="PT-GATE", tasks=[task])
    
    # Start execution in background
    exec_task = asyncio.create_task(
        executor.execute_plan("PT-GATE", "CASE-1", plan, Path("/tmp"))
    )
    
    # Give it a moment to hit the gate
    await asyncio.sleep(0.1)
    
    # Verify it hasn't finished yet (blocked at gate)
    assert not exec_task.done()
    
    # Approve the task
    executor.approve_task("PT-GATE", "T-GATE")
    
    # Wait for completion
    completed, failed = await exec_task
    
    assert len(completed) == 1
    assert completed[0].task_id == "T-GATE"

@pytest.mark.asyncio
async def test_policy_defaults():
    """
    Validates that the default policy is safe (SAFE intensity, requires approval).
    """
    policy = get_active_policy("CASE-DEFAULT")
    
    assert policy.intensity == "SAFE"
    assert policy.human_approval_required is True
    assert "exploit" in policy.forbidden_actions
    assert policy.approval_matrix["HIGH"] is True

@pytest.mark.asyncio
async def test_report_generation(tmp_path):
    """
    Validates that the Report service generates JSON and Markdown reports.
    """
    from api.services.pentest_report import generate_pentest_report
    
    evidence_path = tmp_path / "evidence"
    evidence_path.mkdir()
    
    completed = [
        PentestTaskResult(
            task_id="T1", tool="nmap", status="completed", 
            output="Open ports: 80, 443", duration_seconds=10.5
        )
    ]
    failed = [
        PentestTaskResult(
            task_id="T2", tool="fail_tool", status="failed", 
            error="Connection refused", duration_seconds=1.0
        )
    ]
    
    results = generate_pentest_report("PT-REPORT", "CASE-REPORT", completed, failed, evidence_path)
    
    # Check JSON
    report_dir = evidence_path.parent / "report"
    assert (report_dir / "results.json").exists()
    
    # Check Markdown
    assert (report_dir / "pentest_report.md").exists()
    content = (report_dir / "pentest_report.md").read_text()
    assert "Pentest Report PT-REPORT" in content
    assert "✅ T1 (nmap)" in content
    assert "❌ T2 (fail_tool)" in content

# ============================================================================
# API INTEGRATION TESTS
# ============================================================================

def test_api_start_pentest(test_client, mock_llm_response):
    """
    Tests POST /pentest/start endpoint.
    Mocks the LLM generation to ensure deterministic behavior.
    Mocks the executor to prevent blocking on background tasks.
    """
    with patch("api.services.pentest_planner.safe_llm_call", new_callable=AsyncMock) as mock_llm, \
         patch("api.services.pentest_executor.PentestExecutor.execute_plan", new_callable=AsyncMock) as mock_execute:
        
        mock_llm.return_value = mock_llm_response
        mock_execute.return_value = ([], [])  # Return empty results
        
        payload = {
            "case_id": "API-TEST-CASE",
            "targets": {"domains": ["example.com"]},
            "objectives": "Find open ports",
            "intensity": "SAFE"
        }
        
        response = test_client.post("/pentest/start", json=payload)
        
        assert response.status_code == 200
        data = response.json()
        assert "pentest_id" in data
        assert len(data["tasks"]) == 2
        assert data["tasks"][0]["tool"] == "nmap"
        
        # Verify background task was called (eventually)
        # Note: TestClient runs background tasks before returning response in recent versions,
        # or we might need to wait. But since we mocked it, it should be fast.

def test_api_start_pentest_validation_error(test_client):
    """
    Tests POST /pentest/start with invalid payload.
    """
    payload = {
        "targets": {"domains": ["example.com"]}
        # Missing case_id
    }
    
    response = test_client.post("/pentest/start", json=payload)
    assert response.status_code == 422  # Unprocessable Entity

def test_api_approve_task(test_client):
    """
    Tests POST /pentest/approve endpoint.
    """
    # Mock the executor's approve_task method
    with patch("api.services.pentest_executor.PentestExecutor.approve_task") as mock_approve:
        payload = {
            "pentest_id": "PT-123",
            "task_id": "TASK-001",
            "approved_by": "admin",
            "comment": "Approved for testing"
        }
        
        response = test_client.post("/pentest/approve", json=payload)
        
        assert response.status_code == 200
        assert response.json() == {"status": "approved", "task_id": "TASK-001"}
        mock_approve.assert_called_once_with("PT-123", "TASK-001")

def test_api_approve_task_error(test_client):
    """
    Tests POST /pentest/approve handling of internal errors.
    """
    with patch("api.services.pentest_executor.PentestExecutor.approve_task") as mock_approve:
        mock_approve.side_effect = Exception("Internal Error")
        
        payload = {
            "pentest_id": "PT-ERR",
            "task_id": "TASK-ERR"
        }
        
        response = test_client.post("/pentest/approve", json=payload)
        
        assert response.status_code == 500
        assert "Internal Error" in response.json()["detail"]


@pytest.mark.asyncio
async def test_break_glass_mode(mock_policy):
    """
    Validates that Break Glass mode allows forbidden actions.
    """
    # 1. Create a plan with forbidden action
    forbidden_task = PentestTask(
        task_id="TASK-FORBIDDEN",
        agent="red.exploit",
        tool="exploit",  # Forbidden by default
        command=["exploit", "target"],
        risk_level=PentestRisk.HIGH
    )
    plan = PentestPlan(pentest_id="PT-BG", tasks=[forbidden_task])
    
    # 2. Validate with default policy (should reject)
    # Ensure 'exploit' is in forbidden_actions of mock_policy
    mock_policy.forbidden_actions = ["exploit"]
    mock_policy.break_glass_mode = False
    
    validated_plan = validate_plan(plan, mock_policy)
    assert len(validated_plan.tasks) == 0  # Should be filtered out
    
    # 3. Enable Break Glass
    mock_policy.break_glass_mode = True
    mock_policy.break_glass_justification = "Emergency Authorization"
    
    # 4. Validate again (should allow)
    validated_plan_bg = validate_plan(plan, mock_policy)
    assert len(validated_plan_bg.tasks) == 1
    assert validated_plan_bg.tasks[0].tool == "exploit"
    assert validated_plan_bg.tasks[0].risk_level == PentestRisk.HIGH
